{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir_path = '/Users/sahilgandhi/Datasets/6120-project/data'\n",
    "yelp_data_path = '/Users/sahilgandhi/Datasets/yelp_dataset'\n",
    "\n",
    "tips_data_path = yelp_data_path + '/yelp_academic_dataset_tip.json'\n",
    "\n",
    "#reviews_data_path = 'C:/Users/Aditya/Documents/GitHub/neu/nlp/LDA_Explore/output/useful_reviews_4.json'\n",
    "\n",
    "restaurant_data_path = data_dir_path + '/restaurants.json'\n",
    "\n",
    "stemmed_restaurant_tips_data_path = data_dir_path + '/stemmed_restaurant_tips.json'\n",
    "stemmed_restaurant_reviews_data_path = data_dir_path + '/stemmed_restaurant_reviews.json'\n",
    "pos_tagged_restaurant_reviews_data_path = data_dir_path + '/pos_tagged_restaurant_reviews.json'\n",
    "pos_tagged_restaurant_tips_data_path = data_dir_path + '/pos_tagged_restaurant_tips.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sklearn\n",
    "\n",
    "def load_reviews_by_predicate(filename, predicate_per_row):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            if predicate_per_row(row):\n",
    "                X.append(row['text'])\n",
    "                Y.append(str(int(row['stars'])))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "def predict(model, doc_vector):\n",
    "    return model.transform(doc_vector)\n",
    "\n",
    "def extract_topics(list_of_docs, vectorizer, vectorizer_params, clf, clf_params):\n",
    "    doc_vectorizer = vectorizer(**vectorizer_params)\n",
    "    t0 = time()\n",
    "    doc_vector = doc_vectorizer.fit_transform(list_of_docs)\n",
    "    print(\"vectorizing done in %0.3fs.\" % (time() - t0))\n",
    "    topic_model = clf(**clf_params)\n",
    "    \n",
    "    \n",
    "    t0 = time()\n",
    "    topic_model.fit(doc_vector)\n",
    "    print(\"topic modelling done in %0.3fs.\" % (time() - t0))\n",
    "#     print(f\"\\nTopics in {str(clf)} model:\")\n",
    "#     feature_names = doc_vectorizer.get_feature_names()\n",
    "#     print_top_words(topic_model, feature_names, 10)\n",
    "    return doc_vectorizer, topic_model\n",
    "\n",
    "def sentiment_pos_distribution(list_of_docs):\n",
    "    \"\"\" Look at only adjectives and noun POS-word pairs for the document\n",
    "        Ref:https://link.springer.com/content/pdf/10.1007/978-3-642-22606-9_33.pdf\n",
    "        Ref:https://www.researchgate.net/publication/272863616_Sentiment_Analysis_of_Movie_Reviews_using_POS_tags_and_Term_Frequencies\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    word_pairs = set()\n",
    "    word_pair_features = [defaultdict(lambda: 0)] * len(list_of_docs)\n",
    "    for idx, doc in enumerate(list_of_docs):\n",
    "        for sent in nltk.sent_tokenize(doc):\n",
    "            if sent.strip().count(' ') <= 3: continue  # make sure sent has 3 words atleast\n",
    "            tagged_sent = nltk.word_tokenize(sent)\n",
    "            postags = nltk.pos_tag(tagged_sent)\n",
    "            for word_tag_pairs in nltk.ngrams(postags, 3):\n",
    "                wtp1, wtp2, wtp3 = word_tag_pairs\n",
    "                if ((wtp1[1] == 'JJ' and (wtp2[1] == 'NN' or wtp2[1] == 'NNS')) or\n",
    "                    ((wtp1[1] == 'RB' or wtp1[1] == 'RBR' or wtp1[1] == 'RBS') and (wtp2[1] == 'JJ') and (wtp3[1] != 'NN' or wtp3[1] != 'NNS')) or\n",
    "                    ((wtp1[1] == 'JJ') and (wtp2[1] == 'JJ') and (wtp3[1] != 'NN' or wtp3[1] != 'NNS')) or\n",
    "                    ((wtp1[1] == 'NN' or wtp1[1] == 'NNS') and (wtp2[1] == 'JJ') and (wtp3[1] != 'NN' or wtp3[1] != 'NNS')) or\n",
    "                    ((wtp1[1] == 'RB' or wtp1[1] == 'RBR' or wtp1[1] == 'RBS') and (wtp2[1] == 'VB' or wtp2[1] == 'VBD' or wtp2[1] == 'VBN' or wtp2[1] == 'VBG'))):\n",
    "\n",
    "                    pair_key = f\"{wtp1[0]}-{wtp2[0]}\"\n",
    "                    word_pairs.add(pair_key)\n",
    "                    word_pair_features[idx][pair_key] += 1\n",
    "\n",
    "    print(\"sent features done in %0.3fs.\" % (time() - t0))\n",
    "    return (word_pair_features, list(word_pairs))\n",
    "\n",
    "def generate_pos_distribution_vector(idx, word_pair_features, word_pairs):\n",
    "    x = []\n",
    "    for key in word_pairs:\n",
    "        x.append(word_pair_features[idx][key])\n",
    "    return np.array(x)\n",
    "\n",
    "def topic_features_for_docs(list_of_docs, vectorizer, vectorizer_params, clf, clf_params):\n",
    "    doc_vectorizer, topic_model = extract_topics(list_of_docs, vectorizer, vectorizer_params, clf, clf_params)\n",
    "    X = predict(topic_model, predict(doc_vectorizer, list_of_docs))\n",
    "    return preprocessing.scale(X)\n",
    "\n",
    "def topic_and_sent_features_for_docs(list_of_docs, vectorizer, vectorizer_params, clf, clf_params):\n",
    "    doc_vectorizer, topic_model = extract_topics(list_of_docs, vectorizer, vectorizer_params, clf, clf_params)\n",
    "    word_pair_features, word_pairs = sentiment_pos_distribution(list_of_docs)\n",
    "    X = []\n",
    "    for idx, doc in enumerate(list_of_docs):\n",
    "        topic_vector = predict(topic_model, predict(doc_vectorizer, [doc]))\n",
    "        sent_vector = generate_pos_distribution_vector(idx, word_pair_features, word_pairs)\n",
    "        X.append(np.concatenate((topic_vector.ravel(), sent_vector)))\n",
    "    return preprocessing.scale(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 s, sys: 201 ms, total: 2.9 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs, stars = load_reviews_by_predicate(stemmed_restaurant_reviews_data_path, lambda r: r['business_id'] in ['faPVqws-x-5k2CQKDNtHxw', 'DkYS3arLOhA8si5uUEmHOw', 'fL-b760btOaGa85OJ9ut3w', 'K7lWdNUhCbcnEvI0NhGewg', '5shgJB7a-2_gdnzc0gsOtg', 'ujHiaprwCQ5ewziu0Vi9rw', 'XXW_OFaYQkkGOGniujZFHg'])\n",
    "len(docs), len(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing done in 0.240s.\n",
      "topic modelling done in 7.345s.\n",
      "sent features done in 21.555s.\n",
      "vectorizing done in 0.252s.\n",
      "topic modelling done in 12.569s.\n",
      "sent features done in 22.219s.\n",
      "(2696, 16312) 2696\n",
      "(2696, 16312) 2696\n"
     ]
    }
   ],
   "source": [
    "no_of_topics = 50\n",
    "\n",
    "X_topic_nmf = topic_and_sent_features_for_docs(\n",
    "                docs, CountVectorizer, {'stop_words': 'english'},\n",
    "                NMF, {'n_components': no_of_topics})\n",
    "\n",
    "X_topic_lda = topic_and_sent_features_for_docs(\n",
    "                docs, CountVectorizer, {'stop_words': 'english'},\n",
    "                LatentDirichletAllocation, {'n_components': no_of_topics,\n",
    "                                        'learning_method': 'online',\n",
    "                                        'learning_offset': 50.,\n",
    "                                        'random_state': 0})\n",
    "\n",
    "print(X_topic_nmf.shape, len(stars))\n",
    "print(X_topic_lda.shape, len(stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smv rbf************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahilgandhi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sahilgandhi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sahilgandhi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sahilgandhi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sahilgandhi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def score(clf, X_topic, stars, cv):\n",
    "    print(f\"{clf[1]}\" + \"***\"*20)\n",
    "    scores = cross_val_score(clf[0], X_topic, stars, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print()\n",
    "\n",
    "\n",
    "clfs = [\n",
    "#     (sklearn.svm.SVR(),                     'svr   '),\n",
    "    (sklearn.svm.SVC(kernel='rbf'), 'smv rbf'),\n",
    "    (sklearn.svm.SVC(kernel='poly'), 'smv poly'),\n",
    "    (sklearn.linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial'), 'lg reg'),\n",
    "#     (sklearn.linear_model.LinearRegression(),            'lr    '),\n",
    "#     (sklearn.ensemble.AdaBoostRegressor(),  'en adr'),\n",
    "#     (sklearn.ensemble.BaggingRegressor(),   'en br ')\n",
    "    ]\n",
    "\n",
    "for clf in clfs:\n",
    "    score(clf, X_topic_nmf, stars, 5)\n",
    "\n",
    "print('--'*30)\n",
    "for clf in clfs:\n",
    "    score(clf, X_topic_lda, stars, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
